{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BKwNjLFQ0dMv"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub -q -U\n",
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cache_dir='.'"
      ],
      "metadata": {
        "id": "PWmtBCn801ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wandb -q -U\n",
        "import wandb\n",
        "wandb.login()"
      ],
      "metadata": {
        "id": "KNYVfqSy02XH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import locale\n",
        "def getpreferredencoding(do_setlocale = True):\n",
        "    return \"UTF-8\"\n",
        "locale.getpreferredencoding = getpreferredencoding"
      ],
      "metadata": {
        "id": "xzSqmWsu0799"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install --upgrade pip -q -U\n",
        "!pip install -q datasets\n",
        "!pip install -q -U scipy"
      ],
      "metadata": {
        "id": "_ngYBAve0_OF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_id = \"mistralai/Mistral-7B-Instruct-v0.2\""
      ],
      "metadata": {
        "id": "jIegFgNm1Do0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_path = model_id\n",
        "local_save_path = f\"{cache_dir}/{local_path}\""
      ],
      "metadata": {
        "id": "KtU5nhvR1IXG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install git+https://github.com/huggingface/transformers.git -q -U\n",
        "!pip install -q -U bitsandbytes\n",
        "!pip install -q -U peft\n",
        "!pip install -q -U accelerate"
      ],
      "metadata": {
        "id": "WylfHqoO1I7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import snapshot_download\n",
        "import os\n",
        "\n",
        "def download_model_repo(repo_id, local_dir):\n",
        "    # Download the whole repository to the specified local directory\n",
        "    repo_path = snapshot_download(repo_id=repo_id,\n",
        "                                  cache_dir=local_dir,\n",
        "                                  local_dir=local_dir,\n",
        "                                  local_dir_use_symlinks=False)\n",
        "\n",
        "    print(f\"Repository downloaded to: {repo_path}\")\n",
        "\n",
        "def main():\n",
        "    download_model_repo(model_id, local_save_path)\n",
        "    print()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "id": "UjTm2qQG1Uiy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16\n",
        ")\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    local_save_path,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map='auto',\n",
        "    torch_dtype=torch.float16,\n",
        "    cache_dir=cache_dir)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id,use_fast=True)"
      ],
      "metadata": {
        "id": "VJlvenU81VGu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def print_trainable_parameters(model):\n",
        "    \"\"\"\n",
        "    Prints the number of trainable parameters in the model.\n",
        "    \"\"\"\n",
        "    trainable_params = 0\n",
        "    all_param = 0\n",
        "    for _, param in model.named_parameters():\n",
        "        all_param += param.numel()\n",
        "        if param.requires_grad:\n",
        "            trainable_params += param.numel()\n",
        "    print(\n",
        "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
        "    )"
      ],
      "metadata": {
        "id": "jliYxMhG19nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "config = LoraConfig(\n",
        "    r=128,\n",
        "    lora_alpha=128,\n",
        "    target_modules=[\n",
        "              \"self_attn.q_proj\",\n",
        "              \"self_attn.k_proj\",\n",
        "              \"self_attn.v_proj\",\n",
        "              \"self_attn.o_proj\",\n",
        "              # \"self_attn.rotary_emb.inv_freq\",\n",
        "              # \"mlp.gate_proj\",\n",
        "              # \"mlp.up_proj\",\n",
        "              # \"mlp.down_proj\",\n",
        "              # \"input_layernorm.weight\",\n",
        "              # \"post_attention_layernorm.weight\",\n",
        "              # \"model.norm.weight\",\n",
        "              # \"lm_head.weight\"\n",
        "              ],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "from peft import prepare_model_for_kbit_training\n",
        "\n",
        "model.gradient_checkpointing_enable()\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "ft_model = get_peft_model(\n",
        "    model,\n",
        "    config,\n",
        ")\n",
        "print_trainable_parameters(ft_model)\n"
      ],
      "metadata": {
        "id": "ulM3bEUq2FM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if '<pad>' in tokenizer.get_vocab():\n",
        "    print('<pad> token is in the tokenizer. Using <pad> for pad')\n",
        "    # Set the pad token\n",
        "    tokenizer.pad_token = '<pad>'\n",
        "elif '<unk>' in tokenizer.get_vocab():\n",
        "    print('<unk> token is in the tokenizer. Using unk for pad')\n",
        "    # Set the pad token\n",
        "    tokenizer.pad_token = '<unk>'\n",
        "else:\n",
        "    print(f'Using EOS token, {tokenizer.eos_token}, for padding')\n",
        "    tokenizer.pad_token = tokenizer.eos_token"
      ],
      "metadata": {
        "id": "3AD8m0jA2JXh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.pad_token_id = tokenizer.pad_token_id\n",
        "model.config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "assert model.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
        "\n",
        "print('Tokenizer pad token ID:', tokenizer.pad_token_id)\n",
        "print('Model pad token ID:', model.pad_token_id)\n",
        "print('Model config pad token ID:', model.config.pad_token_id)\n",
        "print('Number of tokens now in tokenizer:', tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "wvlFQlJF2Slc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.generation_config.do_sample = False\n",
        "model.generation_config.temperature = 1.0\n",
        "model.generation_config.top_p = 1.0"
      ],
      "metadata": {
        "id": "eVKebEUw2W4V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TextStreamer\n",
        "from peft import PeftModel\n",
        "\n",
        "system_prompt = 'You are a helpful assistant. You provide succinct answers.'\n",
        "\n",
        "# # For Mistral instruct\n",
        "# system_prompt = ''\n",
        "\n",
        "# Define a stream\n",
        "def stream(user_prompt, model_type, adapter_model):\n",
        "\n",
        "    if model_type == 'base':\n",
        "        eval_model = model\n",
        "    elif model_type == 'fine-tuned':\n",
        "        eval_model = PeftModel.from_pretrained(\n",
        "            model,\n",
        "            adapter_model,\n",
        "        )\n",
        "    elif model_type == 'model_to_push':\n",
        "        eval_model = model_to_push\n",
        "    else:\n",
        "        print('You must set the model_type to base or fine-tuned')\n",
        "        exit()  # or raise an exception\n",
        "\n",
        "    # print(f'Proceeding to inference with {model_type} model')\n",
        "\n",
        "    eval_model.config.use_cache = True\n",
        "\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "\n",
        "    # #For Mistral instruct\n",
        "    # B_SYS, E_SYS = \"\", \"\"\n",
        "\n",
        "    # added_prompt = \"In the context of Touch Rugby and the International Playing Rules set in 2020... \"\n",
        "    added_prompt = ''\n",
        "\n",
        "    # Chat model prompt with system message\n",
        "    prompt = f\"{B_INST} {B_SYS}{system_prompt.strip()}{E_SYS}{added_prompt}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
        "\n",
        "    # # Without system message\n",
        "    # prompt = f\"{B_INST} {added_prompt}{user_prompt.strip()} {E_INST}\\n\\n\"\n",
        "\n",
        "    inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n",
        "\n",
        "    streamer = TextStreamer(tokenizer)\n",
        "\n",
        "    # Despite returning the usual output, the streamer will also print the generated text to stdout.\n",
        "    # _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=50, temperature=0.01\n",
        "    _ = eval_model.generate(**inputs, streamer=streamer, max_new_tokens=500) #if do_sample is False by default\n",
        "\n",
        "def evaluation(model_type, adapter_model=''):\n",
        "questions = [\n",
        "        \"evaluation question\"\n",
        "    ]\n",
        "\n",
        "    answers = [\n",
        "        \"evalauation answer\"\n",
        "    ]\n",
        "\n",
        "    for question, answer in zip(questions, answers):\n",
        "        stream(question, model_type, adapter_model)\n",
        "        print(\"Correct Answer:\", answer)\n",
        "        print('\\n\\n')"
      ],
      "metadata": {
        "id": "-JIIcD2-2Zd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "evaluation(\"base\")\n"
      ],
      "metadata": {
        "id": "SVhTmbb82gaL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_length = 1000\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "data = load_dataset(\"link_to_dataset\")"
      ],
      "metadata": {
        "id": "hMOEu3Wl2mAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"First row of train:\", data['train'][11])\n",
        "print(\"First row of test:\", data['test'][0])"
      ],
      "metadata": {
        "id": "y83VHt5Z3Eez"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import DataLoader, Dataset\n",
        "import torch\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, encodings, response_lengths, input_lengths):\n",
        "        self.encodings = encodings\n",
        "        self.response_lengths = response_lengths\n",
        "        self.input_lengths = input_lengths\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, int):\n",
        "            # print(f\"__getitem__ called with index {idx}\")\n",
        "            item = {key: val[idx].clone().detach() for key, val in self.encodings.items()}\n",
        "            response_start_position = self.input_lengths[idx]\n",
        "            response_end_position = self.input_lengths[idx] + self.response_lengths[idx]\n",
        "        elif isinstance(idx, list):\n",
        "            # print(f\"__getitem__ called with list {idx}\")\n",
        "            item = {key: torch.stack([val[i].clone().detach() for i in idx]) for key, val in self.encodings.items()}\n",
        "            response_start_position = [self.input_lengths[i] for i in idx]\n",
        "            response_end_position = [self.input_lengths[i] + self.response_lengths[i] for i in idx]\n",
        "\n",
        "        # Set labels to be the same as input_ids\n",
        "        item[\"labels\"] = item[\"input_ids\"].clone()\n",
        "\n",
        "        # Create a loss mask that covers only the response tokens\n",
        "        item[\"loss_mask\"] = torch.zeros_like(item[\"input_ids\"])\n",
        "        item[\"loss_mask\"][response_start_position:response_end_position] = 1\n",
        "\n",
        "        # Shift the loss mask to the left by one position\n",
        "        shifted_loss_mask = torch.cat([item[\"loss_mask\"][1:], torch.tensor([0])])\n",
        "        item[\"loss_mask\"] = shifted_loss_mask\n",
        "\n",
        "        # Shift the labels to the left by one position\n",
        "        item[\"labels\"][:-1] = item[\"input_ids\"][1:]\n",
        "\n",
        "        # Replace the token after the response with an EOS token\n",
        "        item[\"labels\"][response_end_position - 1] = 2\n",
        "\n",
        "        # Replace the token after the response with an 1 in the loss mask\n",
        "        item[\"loss_mask\"][response_end_position - 1] = 1\n",
        "\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings[\"input_ids\"])\n"
      ],
      "metadata": {
        "id": "JPdcyU7d3RVd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prepare_dataset(dataset, tokenizer):\n",
        "    B_SYS, E_SYS = \"<<SYS>>\\n\", \"\\n<</SYS>>\\n\\n\"\n",
        "    B_INST, E_INST = \"[INST]\", \"[/INST]\"\n",
        "\n",
        "\n",
        "    formatted_dataset = dataset.map(\n",
        "        lambda x: {\n",
        "            \"input_text\": \"\".join([\n",
        "                f\"{B_INST} {x['prompt'].strip()} {E_INST}\\n\\n\"\n",
        "                f\"{x['completion'].strip()}\",\n",
        "            ]),\n",
        "            \"response_text\": \"\".join([\n",
        "                f\"{x['completion'].strip()}\",\n",
        "            ]),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    # Tokenize the datasets\n",
        "    encodings = tokenizer([dialogue[\"input_text\"] for dialogue in formatted_dataset], truncation=True, padding=True, max_length=data_length, return_tensors='pt', add_special_tokens=True)\n",
        "\n",
        "    # Tokenize the response one by one without padding and special tokens for the purpose of calculating length\n",
        "    response_lengths = [len(tokenizer.encode(dialogue[\"response_text\"], truncation=True, max_length=data_length, padding=False, add_special_tokens=False)) for dialogue in formatted_dataset]\n",
        "\n",
        "    # Tokenize the input one by one without padding and with the initial special token for the purpose of calculating length\n",
        "    total_lengths = [len(tokenizer.encode(dialogue[\"input_text\"], truncation=True, max_length=data_length, padding=False, add_special_tokens=True)) for dialogue in formatted_dataset]\n",
        "    input_lengths = [total_length - response_length for total_length, response_length in zip(total_lengths, response_lengths)]\n",
        "\n",
        "    # Create TextDataset\n",
        "    text_dataset = TextDataset(encodings, response_lengths, input_lengths)\n",
        "\n",
        "    return text_dataset"
      ],
      "metadata": {
        "id": "Woy3UOTX3R4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = prepare_dataset(data['train'], tokenizer)\n",
        "test_dataset = prepare_dataset(data['test'], tokenizer)"
      ],
      "metadata": {
        "id": "6zfue9lR4DFs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3"
      ],
      "metadata": {
        "id": "vnRTaeF34KS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "save_dir = cache_dir + f'/results/{model_id}_{epochs}_epochs_allModules_{data_length}_length_qa_openaidata'\n",
        "print(f'save_dir = {save_dir}')"
      ],
      "metadata": {
        "id": "xkhl_pYa4NhZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import os\n",
        "import transformers\n",
        "\n",
        "# Custom callback to log metrics and save checkpoints\n",
        "class LoggingCallback(transformers.TrainerCallback):\n",
        "    def __init__(self, log_file_path, save_dir):\n",
        "        self.log_file_path = log_file_path\n",
        "        self.save_dir = save_dir\n",
        "\n",
        "    def on_log(self, args, state, control, model=None, logs=None, **kwargs):\n",
        "        with open(self.log_file_path, 'a') as f:\n",
        "            if 'loss' in logs:\n",
        "                f.write(f\"Step: {state.global_step}, Training Loss: {logs['loss']}\\n\")\n",
        "            if 'eval_loss' in logs:\n",
        "                f.write(f\"Step: {state.global_step}, Eval Loss: {logs['eval_loss']}\\n\")\n",
        "            f.flush()  # Force flush the buffered data to file\n",
        "\n",
        "# Log file path\n",
        "log_file_path = os.path.join(cache_dir, \"training_logs.txt\")\n",
        "\n",
        "# Create an instance of the custom callback\n",
        "logging_callback = LoggingCallback(log_file_path, save_dir)\n"
      ],
      "metadata": {
        "id": "whvbBVGe4THG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomTrainer(transformers.Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        # Define the number of tokens you want to display\n",
        "        num_tokens = 25  # This displays info on the actual and predicted tokens at the end of each sequence.\n",
        "\n",
        "        labels = inputs.pop(\"labels\")\n",
        "        loss_mask = inputs.pop(\"loss_mask\")\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "        logits = outputs.logits\n",
        "\n",
        "        # Check for NaN in logits and labels\n",
        "        if torch.isnan(logits).any():\n",
        "            print(\"NaN detected in logits\")\n",
        "            print(logits)\n",
        "\n",
        "        # Convert logits to probabilities using softmax function\n",
        "        probs = nn.functional.softmax(logits, dim=-1)\n",
        "\n",
        "        # Get the most probable tokens\n",
        "        predicted_token_ids = torch.argmax(probs, dim=-1)\n",
        "\n",
        "        # Compute the loss\n",
        "        loss_fct = nn.CrossEntropyLoss(reduction='none')\n",
        "        losses = loss_fct(logits.view(-1, self.model.config.vocab_size), labels.view(-1))\n",
        "\n",
        "        # Reshaping the losses to have dimensions [batch_size, seq_length]\n",
        "        losses = losses.view(-1, inputs['input_ids'].size(1))\n",
        "\n",
        "        # Apply the loss mask\n",
        "        masked_loss = losses * loss_mask\n",
        "\n",
        "        # Check for NaN in losses and zero in loss_mask.sum()\n",
        "        if torch.isnan(losses).any():\n",
        "            print(\"NaN detected in losses\")\n",
        "            # print(losses)\n",
        "\n",
        "        if loss_mask.sum() == 0:\n",
        "            print(\"Sum of loss_mask is zero\")\n",
        "            return (torch.tensor(0).to(loss_mask.device), outputs) if return_outputs else torch.tensor(0).to(loss_mask.device)  # Early return\n",
        "\n",
        "        # Aggregate the masked losses\n",
        "        loss = masked_loss.sum() / (loss_mask.sum() + 1e-9)\n",
        "\n",
        "        # Print formatted tokens\n",
        "        batch_size, seq_length = inputs['input_ids'].size()\n",
        "\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "    def get_train_dataloader(self):\n",
        "      train_dataset = self.train_dataset\n",
        "      data_collator = self.data_collator\n",
        "\n",
        "      dataloader_params = {\n",
        "          \"batch_size\": self.args.train_batch_size,\n",
        "          \"collate_fn\": data_collator,\n",
        "          \"num_workers\": self.args.dataloader_num_workers,\n",
        "          \"pin_memory\": self.args.dataloader_pin_memory,\n",
        "      }\n",
        "\n",
        "      if not isinstance(train_dataset, torch.utils.data.IterableDataset):\n",
        "          dataloader_params[\"sampler\"] = self._get_train_sampler()\n",
        "          dataloader_params[\"drop_last\"] = self.args.dataloader_drop_last\n",
        "\n",
        "      return DataLoader(train_dataset, **dataloader_params)\n",
        "\n",
        "    def get_eval_dataloader(self, eval_dataset=None):\n",
        "      eval_dataset = eval_dataset if eval_dataset is not None else self.eval_dataset\n",
        "      if eval_dataset is None:\n",
        "          raise ValueError(\"Trainer: evaluation requires an eval_dataset.\")\n",
        "\n",
        "      data_collator = self.data_collator\n",
        "\n",
        "      # Parameters for the DataLoader\n",
        "      dataloader_params = {\n",
        "          \"batch_size\": self.args.eval_batch_size,\n",
        "          \"collate_fn\": data_collator,\n",
        "          \"num_workers\": self.args.dataloader_num_workers,\n",
        "          \"pin_memory\": self.args.dataloader_pin_memory,\n",
        "      }\n",
        "\n",
        "      if not isinstance(eval_dataset, torch.utils.data.IterableDataset):\n",
        "          dataloader_params[\"sampler\"] = self._get_eval_sampler(eval_dataset)\n",
        "          dataloader_params[\"drop_last\"] = False\n",
        "      return DataLoader(eval_dataset, **dataloader_params)\n"
      ],
      "metadata": {
        "id": "iT1GOghW_kTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomDataCollator:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, batch):\n",
        "\n",
        "        input_ids = torch.stack([item['input_ids'] for item in batch])\n",
        "        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n",
        "        labels = torch.stack([item['labels'] for item in batch])\n",
        "        loss_mask = torch.stack([item['loss_mask'] for item in batch])\n",
        "\n",
        "        return {\n",
        "            'input_ids': input_ids,\n",
        "            'attention_mask': attention_mask,\n",
        "            'labels': labels,\n",
        "            'loss_mask': loss_mask\n",
        "        }\n",
        "\n",
        "data_collator = CustomDataCollator(tokenizer)"
      ],
      "metadata": {
        "id": "3wdfdgck_nWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = CustomTrainer(\n",
        "    model=ft_model,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    args=transformers.TrainingArguments(\n",
        "        # max_steps=3,\n",
        "        num_train_epochs=epochs,\n",
        "        per_device_train_batch_size=2,\n",
        "        per_device_eval_batch_size=2,\n",
        "        gradient_accumulation_steps=1,\n",
        "        evaluation_strategy=\"steps\",\n",
        "        max_grad_norm=1,\n",
        "        warmup_ratio=0.1,\n",
        "        eval_steps=0.2,\n",
        "        learning_rate=1e-4,\n",
        "        fp16=True,\n",
        "        logging_steps=1,\n",
        "        output_dir=save_dir,\n",
        "        optim=\"adamw_torch\",\n",
        "        # lr_scheduler_type='cosine',\n",
        "        lr_scheduler_type='constant',\n",
        "        save_steps=0.2, #not supported for 4-bit models\n",
        "        hub_private_repo=True\n",
        "    ),\n",
        "    data_collator=data_collator,\n",
        "    callbacks=[logging_callback],\n",
        ")\n",
        "ft_model.config.use_cache = False"
      ],
      "metadata": {
        "id": "GENXeawr_raY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "G3YhjAAB_tuk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_losses = []\n",
        "eval_losses = []\n",
        "train_steps = []\n",
        "eval_steps = []\n",
        "\n",
        "for entry in trainer.state.log_history:\n",
        "    if 'loss' in entry:\n",
        "        train_losses.append(entry['loss'])\n",
        "        train_steps.append(entry['step'])\n",
        "    if 'eval_loss' in entry:\n",
        "        eval_losses.append(entry['eval_loss'])\n",
        "        eval_steps.append(entry['step'])\n",
        "\n",
        "plt.plot(train_steps, train_losses, label='Train Loss')\n",
        "plt.plot(eval_steps, eval_losses, label='Eval Loss')\n",
        "plt.xlabel('Steps')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "dKd0KRid_wso"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_dir = save_dir + '/checkpoint-54'\n",
        "print(f'Running evaluation using the adapter at: {adapter_dir}\\n\\n')\n",
        "evaluation(\"fine-tuned\", adapter_dir)"
      ],
      "metadata": {
        "id": "kkcR5wPj_z1m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_to_push = save_dir + '/checkpoint-1028'\n",
        "\n",
        "print(f'Defining the parameters to push adapters for checkpoint: {adapter_to_push}\\n\\n')\n",
        "\n",
        "# Extract the last portion of the base_model\n",
        "model_name = model_id.split(\"/\")[-1]\n",
        "fine_tuned_tag = 'sft-test-push'\n",
        "\n",
        "# Define the save and push paths\n",
        "adapter_model_name = f\"Augustya07/{model_name}-{fine_tuned_tag}-adapters\"\n",
        "new_model = f\"Augustya07/{model_name}-{fine_tuned_tag}\"\n",
        "print(f\"Setting up for pushing to repos:\\n- {adapter_model_name}\\n- {new_model}\")"
      ],
      "metadata": {
        "id": "4K_Y2KId_4Ww"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.config._name_or_path = model_id\n",
        "\n",
        "model_to_push = PeftModel.from_pretrained(\n",
        "    model,\n",
        "    adapter_to_push,\n",
        ")"
      ],
      "metadata": {
        "id": "jVHP0P41_789"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "local_adapter_model = adapter_model_name + '-local'\n",
        "model_to_push.save_pretrained(local_adapter_model, token=True)"
      ],
      "metadata": {
        "id": "jlpATw30_-ah"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_to_push.push_to_hub(adapter_model_name, token=True, safe_serialization=True)"
      ],
      "metadata": {
        "id": "BL9i0YmGAEo6"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}